{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('fastbook': conda)",
   "metadata": {
    "interpreter": {
     "hash": "072c09975d5648d66d7606e77e2f2dcef0eaf0858c8c7901e914801bb1828839"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Week 8 - Chapter 4A\n",
    "> Exercises from Chapter 4A.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter, exercise]\n",
    "- image: images/chart-preview.png"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "1. Explain how the \"pixel similarity\" approach to classifying digits works.\n",
    "    - Finder gennemsnitlige pixel værdi af det vi prøver at finde, sammenligner pixel værdierne for et digit med vores average.\n",
    "    \n",
    "1. What is a list comprehension?\n",
    "    - Create one now that selects odd numbers from a list and doubles them.\n",
    "        [i * 2 for i in numbers if x % 2 == 1]\n",
    "\n",
    "1. What is a \"rank-3 tensor\"?\n",
    "    - A single 3 dimensional tensor\n",
    "\n",
    "1. What are RMSE and L1 norm?\n",
    "    - Root mean squared error eller L2:\n",
    "    - Mean absolute difference:\n",
    "    \n",
    "1. Create a 3×3 tensor or array containing the numbers from 1 to 9.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "\n",
    "ten = tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]])"
   ]
  },
  {
   "source": [
    "- Double it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 2,  4,  6],\n",
       "        [ 8, 10, 12],\n",
       "        [14, 16, 18]])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "ten = ten * 2\n",
    "ten\n"
   ]
  },
  {
   "source": [
    "- Select the bottom-right four numbers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[10, 12],\n",
       "        [16, 18]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "ten[1:,1:]"
   ]
  },
  {
   "source": [
    "    6. What is broadcasting?  \n",
    "    - Hvis to tensor er af forskellige ranks, kan vi bruge broadcasting til at expande den mindre ranket tensor til samme rank, som den større tensor. Det er vigtigt at forstå at PyTorch ikke faktisk kopier billederne til at opnå ranken, men lader som om at tensoren er af den størrelse. Derved undgås der at allokeres hukommelse til det. Dette muliggør, at PyTorch kan bruge matematiske funktioner på de 2 tensors / array f.eks. at gange dem eller tage gennemsnittet.\n",
    "    \n",
    "1. Are metrics generally calculated using the training set, or the validation set? Why?\n",
    "    - Det gøres på validation sættet, da vi måler en models performance på deres evne til at kategorisere ikke før set data.\n",
    "\n",
    "1. What is SGD?\n",
    "    - Stochastic Gradient Descent\n",
    "    - Det er en iterativ måde, at optimere vores model via. gradients. Afhængig af vores Learning Rate, tager vi små skridt mod det bedste fit.\n",
    "\n",
    "1. What are the seven steps in SGD for machine learning?\n",
    "\n",
    "    - Initialize the parameters\n",
    "    - Calculate the predictions\n",
    "    - Calculate the loss\n",
    "    - Calculate the gradients\n",
    "    - Step the weights\n",
    "    - Repeat the Process\n",
    "    - Stop\n",
    "\n",
    "    ![The Seven Steps](assignments/the-seven-steps.jpg)    \n",
    "\n",
    "1. How do we initialize the weights in a model?\n",
    "    - Random\n",
    "    - Pretrained model (Transfer learning).\n",
    "\n",
    "1. What is \"loss\"?\n",
    "    - Mængde af fejl i vores model, som skal minimeres. Loss er højere, hvis modellen tager fejl især, hvis den også er meget sikker på fejlen.\n",
    "\n",
    "1. Why can't we always use a high learning rate?\n",
    "    - Fordi man vil overshoot ens minimale loss eller nogen gange blive værre, da den tager for store skridt.\n",
    "\n",
    "1. What is a \"gradient\"?\n",
    "    - Hvor stejlt / hældningen på det punkt i kurven.\n",
    "\n",
    "1. Why can't we use accuracy as a loss function?\n",
    "    - Fordi accuracy er ikke godt nok til en model. Loss er en bedre repræsentation matematisk for fejl.\n",
    "\n",
    "1. What is the difference between a loss function and a metric?\n",
    "    - Loss function bruges til at justere vægte mens metrics er det vi selv måler på for at se kvaliteten af vores model.\n",
    "\n",
    "1. What is the function to calculate new weights using a learning rate?\n",
    "    - Backpropagation.\n",
    "    \n",
    "1. What does the backward method do?\n",
    "    - Justere vægte igennem netværket, starter ved head af modellen.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}